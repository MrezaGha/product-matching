{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es geht darum die verschiedenen Sentece Transformer anzuwenden, um heruaszufinden welche besser funktioniert\n",
    "# äEs soll darum gehen die 2 Größten und best performanten miteinander zu vergleichen und eine ST die auf deutsche Daten traniert wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_amazon = r'daten/amazon_datenbereinigt.xlsx'\n",
    "df_amazon = pd.read_excel(data_amazon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_otto = r'daten/otto_datenbereinigt.xlsx'\n",
    "df_otto = pd.read_excel(data_otto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separator = \" | \"  # Trennzeichen\n",
    "# Amazon data\n",
    "df_amazon_sentence = df_amazon.apply(lambda row: ''.join(row[col] for col in ['NAME', 'DESCRIPTION'] \n",
    "                                                          if not pd.isna(row[col])), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otto data\n",
    "df_otto_sentence = df_otto.apply(lambda row: ''.join(row[col] for col in ['NAME','DESCRIPTION'] \n",
    "                                                      if not pd.isna(row[col])), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 73.0kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 46.8kB/s]\n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 6.86MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 50.0kB/s]\n",
      "config.json: 100%|██████████| 573/573 [00:00<00:00, 399kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 134M/134M [00:04<00:00, 32.7MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 369kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.17MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 68.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 86.3kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 141kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_zweitbeste  = SentenceTransformer('all-MiniLM-L12-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_embeddings = model_zweitbeste.encode(df_amazon_sentence, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otto_embeddings = model_zweitbeste.encode(df_otto_sentence, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazon_product</th>\n",
       "      <th>otto_product_1</th>\n",
       "      <th>otto_product_2</th>\n",
       "      <th>otto_product_3</th>\n",
       "      <th>otto_product_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schöffel fleecejacke fleece jacke pelham megal...</td>\n",
       "      <td>dare2b outdoorjacke ascension jacket wasserdic...</td>\n",
       "      <td>hummel fleecejackein dieser robusten hmlauthen...</td>\n",
       "      <td>schöffel skijacke herren skijacke ski jacket t...</td>\n",
       "      <td>schoeffel outdoorjackedie winddichte jacke 3in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas originals outdoorjacke hooded premium l...</td>\n",
       "      <td>adidas five ten wanderschuhdie adidas terrex a...</td>\n",
       "      <td>adidas sportswear winterjacke bsc 3 streifen h...</td>\n",
       "      <td>adidas performance regenjacke utilitas rainrdy...</td>\n",
       "      <td>adidas sportswear winterjacke herren daunenwes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brax stretch hose styleleeeine sportive hose m...</td>\n",
       "      <td>aatrx schlaghose sexy damen yogahose mit niedr...</td>\n",
       "      <td>ellesse jogginghose hose ellesse queenstown 1 ...</td>\n",
       "      <td>cecil dehnbund hose elastikbund mit tunnelzugc...</td>\n",
       "      <td>eevi stoffhose eevi hose ceremony graudie eleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shirtracer t shirt vorsicht vor dem frauchen p...</td>\n",
       "      <td>dogo go home sneaker vegandieser trendige snea...</td>\n",
       "      <td>dogo where am i  sneaker vegandieser zauberhaf...</td>\n",
       "      <td>tini shirts print shirt pit bull hunde kinder ...</td>\n",
       "      <td>shirtracer t shirt gästebuch 70 geburtstag gäs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vidaxl pouf gartenstuhl auflagen 4 stk bedruck...</td>\n",
       "      <td>meisemöbel polsterbett polsterbett frieda mit ...</td>\n",
       "      <td>vidaxl bettgestell massivholzbett mit kopfteil...</td>\n",
       "      <td>vidaxl bettgestell bettgestell blau 120x200 cm...</td>\n",
       "      <td>ormromra kletterseiloutdoor seil durchmesser s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f4nt4stic t shirt star wars the mandalorian di...</td>\n",
       "      <td>f4nt4stic t shirt long cut t shirt star wars t...</td>\n",
       "      <td>star wars t shirthochwertiges t shirt von der ...</td>\n",
       "      <td>star wars t shirthochwertiges t shirt von der ...</td>\n",
       "      <td>star wars print shirt star wars han solo t shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cmp strickfleecejacke cmp hybrid jacket fix ho...</td>\n",
       "      <td>klättermusen outdoorjacke glaser jacket menrob...</td>\n",
       "      <td>cmp fahrradjacke woman jacketextraleichte fahr...</td>\n",
       "      <td>mfh langarmshirt sweatshirt schwarz pc 340gm m...</td>\n",
       "      <td>tatonka softshelljacke jesper hooded jacket me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gloria hundeleine hundeleine gloria schwarz 1 ...</td>\n",
       "      <td>almgwand trachtenshirt langarmshirt geisleralm...</td>\n",
       "      <td>kerbl draht weidezaunseil elektroseil profi pe...</td>\n",
       "      <td>ulla popken schlupfhose hose sophie karomuster...</td>\n",
       "      <td>gant langarmhemd karohemddas gant herren hemd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>erima sportsocken 3er pack 5 c socksmit der 5 ...</td>\n",
       "      <td>100% sportsockenphochwertiges sockenpaar im 10...</td>\n",
       "      <td>reebok classic sportsocken classic footwear in...</td>\n",
       "      <td>soliver 78 jeans ankle jeans izabell  skinny f...</td>\n",
       "      <td>erima trainingsshortserima premium one 20 shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ulla popken langarmshirt shirt classic crepe k...</td>\n",
       "      <td>ambria 34 arm shirt shirt 1 tlgdas shirt wirkt...</td>\n",
       "      <td>uana langarmshirt einfarbiges lockeres langarm...</td>\n",
       "      <td>we fashion t shirt 1 tlggesteiftes rollkragens...</td>\n",
       "      <td>fldhood t shirt damen casual off shoulder slim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      amazon_product  \\\n",
       "0  schöffel fleecejacke fleece jacke pelham megal...   \n",
       "1  adidas originals outdoorjacke hooded premium l...   \n",
       "2  brax stretch hose styleleeeine sportive hose m...   \n",
       "3  shirtracer t shirt vorsicht vor dem frauchen p...   \n",
       "4  vidaxl pouf gartenstuhl auflagen 4 stk bedruck...   \n",
       "5  f4nt4stic t shirt star wars the mandalorian di...   \n",
       "6  cmp strickfleecejacke cmp hybrid jacket fix ho...   \n",
       "7  gloria hundeleine hundeleine gloria schwarz 1 ...   \n",
       "8  erima sportsocken 3er pack 5 c socksmit der 5 ...   \n",
       "9  ulla popken langarmshirt shirt classic crepe k...   \n",
       "\n",
       "                                      otto_product_1  \\\n",
       "0  dare2b outdoorjacke ascension jacket wasserdic...   \n",
       "1  adidas five ten wanderschuhdie adidas terrex a...   \n",
       "2  aatrx schlaghose sexy damen yogahose mit niedr...   \n",
       "3  dogo go home sneaker vegandieser trendige snea...   \n",
       "4  meisemöbel polsterbett polsterbett frieda mit ...   \n",
       "5  f4nt4stic t shirt long cut t shirt star wars t...   \n",
       "6  klättermusen outdoorjacke glaser jacket menrob...   \n",
       "7  almgwand trachtenshirt langarmshirt geisleralm...   \n",
       "8  100% sportsockenphochwertiges sockenpaar im 10...   \n",
       "9  ambria 34 arm shirt shirt 1 tlgdas shirt wirkt...   \n",
       "\n",
       "                                      otto_product_2  \\\n",
       "0  hummel fleecejackein dieser robusten hmlauthen...   \n",
       "1  adidas sportswear winterjacke bsc 3 streifen h...   \n",
       "2  ellesse jogginghose hose ellesse queenstown 1 ...   \n",
       "3  dogo where am i  sneaker vegandieser zauberhaf...   \n",
       "4  vidaxl bettgestell massivholzbett mit kopfteil...   \n",
       "5  star wars t shirthochwertiges t shirt von der ...   \n",
       "6  cmp fahrradjacke woman jacketextraleichte fahr...   \n",
       "7  kerbl draht weidezaunseil elektroseil profi pe...   \n",
       "8  reebok classic sportsocken classic footwear in...   \n",
       "9  uana langarmshirt einfarbiges lockeres langarm...   \n",
       "\n",
       "                                      otto_product_3  \\\n",
       "0  schöffel skijacke herren skijacke ski jacket t...   \n",
       "1  adidas performance regenjacke utilitas rainrdy...   \n",
       "2  cecil dehnbund hose elastikbund mit tunnelzugc...   \n",
       "3  tini shirts print shirt pit bull hunde kinder ...   \n",
       "4  vidaxl bettgestell bettgestell blau 120x200 cm...   \n",
       "5  star wars t shirthochwertiges t shirt von der ...   \n",
       "6  mfh langarmshirt sweatshirt schwarz pc 340gm m...   \n",
       "7  ulla popken schlupfhose hose sophie karomuster...   \n",
       "8  soliver 78 jeans ankle jeans izabell  skinny f...   \n",
       "9  we fashion t shirt 1 tlggesteiftes rollkragens...   \n",
       "\n",
       "                                      otto_product_4  \n",
       "0  schoeffel outdoorjackedie winddichte jacke 3in...  \n",
       "1  adidas sportswear winterjacke herren daunenwes...  \n",
       "2  eevi stoffhose eevi hose ceremony graudie eleg...  \n",
       "3  shirtracer t shirt gästebuch 70 geburtstag gäs...  \n",
       "4  ormromra kletterseiloutdoor seil durchmesser s...  \n",
       "5  star wars print shirt star wars han solo t shi...  \n",
       "6  tatonka softshelljacke jesper hooded jacket me...  \n",
       "7  gant langarmhemd karohemddas gant herren hemd ...  \n",
       "8  erima trainingsshortserima premium one 20 shor...  \n",
       "9  fldhood t shirt damen casual off shoulder slim...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modell Laden https://aclanthology.org/2022.bucc-1.9.pdf quelle !!!\n",
    "# zwei beste model im vergleich \n",
    "dim = df_otto_embeddings.shape[1]  # Vektor-Dimension\n",
    "index = hnswlib.Index(space='l2', dim=dim)# l2 für Euklid\n",
    "\n",
    "index.init_index(max_elements=len(df_otto), ef_construction=200, M=16)\n",
    "\n",
    "index.add_items(df_otto_embeddings)\n",
    "\n",
    "index.save_index('_2bestemodell_hnswlib_index_otto.bin')\n",
    "index.set_ef(100) \n",
    "\n",
    "similar_items = []\n",
    "for i, embedding in enumerate(df_amazon_embeddings):\n",
    "    # Abfragen der fünf nächsten Nachbarn (das eigentliche Produkt eingeschlossen 1+)\n",
    "    labels, distances = index.knn_query(embedding, k=5)\n",
    "    # Speichern der ähnlichsten Produkte\n",
    "    similar_products = [df_otto_sentence[j] for j in labels[0]]\n",
    "    similar_items.append(similar_products)\n",
    "\n",
    "df = pd.DataFrame(similar_items, columns=['amazon_product',\n",
    "                                          'otto_product_1', \n",
    "                                          'otto_product_2',\n",
    "                                          'otto_product_3',\n",
    "                                          'otto_product_4'\n",
    "                                          ])\n",
    "df.to_excel(\"daten_sentence_tranformer_im_vergleich/all_distil_robertav1_2beste_amazon_otto_daten.xlsx\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_examples(df, num_negatives_per_positive=1):\n",
    "    \"\"\"\n",
    "    Ähnliche Anzahl von negativen Beispielen, basierend auf den vorhandenen positiven Beispielen genrerien.\n",
    "    Jedes negative Beispiel ist ein Paar von Produkten, die nicht übereinstimmen.\n",
    "    \n",
    "    :param df: DataFrame mit den positiven Beispielen.\n",
    "    :param num_negatives_per_positive: Anzahl der negativen Beispiele, die pro positivem Beispiel generiert werden sollen.\n",
    "    :return: DataFrame mit den negativen Beispielen.\n",
    "    \"\"\"\n",
    "    negative_examples = []\n",
    "    product_names = df['amazon_product_name'].tolist() + df['otto_variation_name'].tolist()\n",
    "    \n",
    "    for _, row in df.iterrows(): # der index selbst wird ignoriert \n",
    "        for _ in range(num_negatives_per_positive):\n",
    "            # zuffälig zwei Produkte nehmen, die nicht paare sind \n",
    "            while True:\n",
    "                product1 = random.choice(product_names)\n",
    "                product2 = random.choice(product_names)\n",
    "                \n",
    "                # hierit wird sichergestellt, dass product1 und product2 nicht übereinstimmen und nicht in den positiven Beispielen sind\n",
    "                if product1 != product2 and not ((df['amazon_product_name'] == product1) & (df['otto_variation_name'] == product2)).any():\n",
    "                    break\n",
    "            \n",
    "            negative_examples.append({'amazon_product_name': product1, 'otto_variation_name': product2})\n",
    "    \n",
    "    return pd.DataFrame(negative_examples)\n",
    "\n",
    "# schwere Negative bsp bauen!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_harder_negative_examples(df, num_negatives_per_positive=1):\n",
    "    \"\"\"\n",
    "    Generiert eine ähnliche Anzahl von 'schweren' negativen Beispielen basierend auf den vorhandenen positiven Beispielen.\n",
    "    'Schwere' negative Beispiele sind solche, bei denen die Produkte ähnlich, aber nicht identisch sind.\n",
    "    \n",
    "    :param df: DataFrame mit den positiven Beispielen und zusätzlichen Informationen wie Kategorien.\n",
    "    :param num_negatives_per_positive: Anzahl der negativen Beispiele pro positivem Beispiel.\n",
    "    :return: DataFrame mit den negativen Beispielen.\n",
    "    \"\"\"\n",
    "    negative_examples = []\n",
    "    categories = df['kategorie_bezeichnung'].unique()  # Eindeutige Kategorien extrahieren\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for _ in range(num_negatives_per_positive):\n",
    "            # Wählen Sie eine Kategorie zufällig aus, die dem aktuellen positiven Beispiel entspricht\n",
    "            current_category = row['kategorie_bezeichnung']\n",
    "            \n",
    "            # Produkte innerhalb der gleichen Kategorie, aber nicht das gleiche Produkt\n",
    "            similar_products = df[(df['kategorie_bezeichnung'] == current_category) & \n",
    "                                  (df['amazon_product_name'] != row['amazon_product_name']) &\n",
    "                                  (df['otto_variation_name'] != row['otto_variation_name'])]\n",
    "            \n",
    "            # Wenn es ähnliche Produkte gibt, wählen Sie eines zufällig als negatives Beispiel\n",
    "            if not similar_products.empty:\n",
    "                negative_example = similar_products.sample(1)  # Wählen Sie ein zufälliges ähnliches Produkt\n",
    "                product1 = row['amazon_product_name']\n",
    "                product2 = negative_example['otto_variation_name'].values[0]  # Nehmen Sie den Namen des Otto-Produkts\n",
    "                \n",
    "                negative_examples.append({'amazon_product_name': product1, 'otto_variation_name': product2})\n",
    "\n",
    "    return pd.DataFrame(negative_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle(r'daten/otto_amazon_variation_pairs.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_embeddings_1 = model_zweitbeste.encode(df['amazon_product_name'].tolist())\n",
    "positive_embeddings_2 = model_zweitbeste.encode(df['otto_variation_name'].tolist())\n",
    "# Absolute Differenz als Features berechnen \n",
    "positive_features = np.abs(positive_embeddings_1 - positive_embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = generate_harder_negative_examples(df)\n",
    "negative_embeddings_1 = model_zweitbeste.encode(negative_df['amazon_product_name'].tolist()) #eventuell nochmal mit batchsize machen\n",
    "negative_embeddings_2 = model_zweitbeste.encode(negative_df['otto_variation_name'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101269,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91     12556\n",
      "         1.0       0.92      0.90      0.91     12762\n",
      "\n",
      "    accuracy                           0.91     25318\n",
      "   macro avg       0.91      0.91      0.91     25318\n",
      "weighted avg       0.91      0.91      0.91     25318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_features = np.abs(negative_embeddings_1 - negative_embeddings_2)\n",
    "# Features und Labels zusammenführen Positive Features\n",
    "positive_labels = np.ones(len(positive_features))\n",
    "negative_labels = np.zeros(len(negative_features))\n",
    "# Features und Labels zusammenführen\n",
    "X = np.concatenate([positive_features, negative_features], axis=0)\n",
    "y = np.concatenate([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "# Trainings- und Testsets aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "\"\"\" # SVM trainieren\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation \n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred)) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.74      0.75     12556\n",
      "         1.0       0.75      0.76      0.75     12762\n",
      "\n",
      "    accuracy                           0.75     25318\n",
      "   macro avg       0.75      0.75      0.75     25318\n",
      "weighted avg       0.75      0.75      0.75     25318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_tree = clf_tree.fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91     12556\n",
      "         1.0       0.94      0.88      0.91     12762\n",
      "\n",
      "    accuracy                           0.91     25318\n",
      "   macro avg       0.91      0.91      0.91     25318\n",
      "weighted avg       0.91      0.91      0.91     25318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf_GDC = SGDClassifier(loss=\"modified_huber\", max_iter=3000, tol=1e-3)\n",
    "clf_GDC.fit(X_train, y_train)\n",
    "y_pred = clf_GDC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.96      0.90     12556\n",
      "         1.0       0.96      0.82      0.88     12762\n",
      "\n",
      "    accuracy                           0.89     25318\n",
      "   macro avg       0.90      0.89      0.89     25318\n",
      "weighted avg       0.90      0.89      0.89     25318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf  = clf_rf.fit(X_train, y_train)\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modell/zweite_svm_modell_hard_Negativ_all-MiniLM-L12-v2.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modell speichern\n",
    "from joblib import dump\n",
    "dump(clf, 'modell/zweite_svm_modell_hard_Negativ_all-MiniLM-L12-v2.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuent_german = SentenceTransformer('all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weitere Modell um Sentence transformer modelle miteinander anhand der Ergebnisse zu vergleichen: auf deutsche Daten fine tuent \n",
    "df_amazon_embeddings = model_finetuent_german.encode(df_amazon_sentence, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_otto_embeddings \u001b[39m=\u001b[39m model_finetuent_german\u001b[39m.\u001b[39;49mencode(df_otto_sentence, batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:284\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    281\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    283\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 284\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    287\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m: output_tokens, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: features[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    836\u001b[0m     embedding_output,\n\u001b[1;32m    837\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    838\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    839\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    840\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    841\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    842\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    843\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    844\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    845\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/p-matching/product-matching/bachelorarbeit/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:260\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    257\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    258\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> 260\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_otto_embeddings = model_finetuent_german.encode(df_otto_sentence, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim = df_otto_embeddings.shape[1]  # Vektor-Dimension\n",
    "index = hnswlib.Index(space='l2', dim=dim)# l2 für Euklid\n",
    "\n",
    "index.init_index(max_elements=len(df_otto), ef_construction=200, M=16)\n",
    "\n",
    "index.add_items(df_otto_embeddings)\n",
    "\n",
    "index.save_index('_deutsche_bestemodell_hnswlib_index_otto.bin')\n",
    "index.set_ef(100) \n",
    "\n",
    "similar_items = []\n",
    "for i, embedding in enumerate(df_amazon_embeddings):\n",
    "    # Abfragen der fünf nächsten Nachbarn (das eigentliche Produkt eingeschlossen 1+)\n",
    "    labels, distances = index.knn_query(embedding, k=5)\n",
    "    # Speichern der ähnlichsten Produkte\n",
    "    similar_products = [df_otto_sentences[j] for j in labels[0]]\n",
    "    similar_items.append(similar_products)\n",
    "\n",
    "df = pd.DataFrame(similar_items, columns=['amazon_product',\n",
    "                                          'otto_product_1', \n",
    "                                          'otto_product_2',\n",
    "                                          'otto_product_3',\n",
    "                                          'otto_product_4'\n",
    "                                          ])\n",
    "df.to_excel(\"daten_sentence_tranformer_im_vergleich/T-Systems-onsite_deutschmodell_amazon_otto_daten.xlsx\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Präziste Model! Quelle : https://www.sbert.net/docs/pretrained_models.html\n",
    "model_beste = sentence_transformers('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bachelorarbeit': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a6dcd5c1c1941ffa60e357ee1f7133a2c552a4a7fb1e2a5b3b962f361695b81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
